<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">

  <title>GarmentDiffusion</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GarmentDiffusion: 3D Garment Pattern Generation with Multi-modal Diffusion Transformers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=haWm-DoAAAAJ" target="_blank">Xinyu Li<sup>1,2</sup></a>,</span>
<!--                 Xinyu Li<sup>1,2</sup>,</span> -->
                <span class="author-block">
                  <a href="xxx" target="_blank">Qi Yao<sup>2</sup></a>,</span>
                  <!-- Qi Yao<sup>2</sup>,</span> -->
                  <span class="author-block">
                    <a href="xxx" target="_blank">Yuanda Wang<sup>2</sup></a>
                    <!-- Yuanda Wang<sup>2</sup> -->
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Zhejiang University&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>Shenfu Research</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block" style="color: #EC9243; font-weight: bold;">IJCAI 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2504.21476" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="ai ai-arxiv"></i>
                          </span>
                          <span>ArXiv</span>
                        </a>
                      </span>

                      <!-- Paper Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2504.21476" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="ai ai-arxiv"></i>
                          </span>
                          <span>Paper</span>
                        </a>
                      </span>

                      <!-- Github link -->
                      <span class="link-block">
                        <a href="https://github.com/Shenfu-Research/Garment-Diffusion" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fab fa-github"></i>
                          </span>
                          <span>Code</span>
                        </a>
                      </span>

                      <!-- Dataset link -->
                      <span class="link-block">
                        <a href="https://github.com/Shenfu-Research/Garment-Diffusion" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fab fa-github"></i>
                          </span>
                          <span>Dataset</span>
                        </a>
                      </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Garment sewing patterns are fundamental design elements that bridge the gap between garment design and practical manufacturing. 
            The generative modeling of sewing patterns is crucial for creating diverse and innovative garment designs. 
            However, existing approaches are limited either by reliance on a single input modality or by suboptimal generation efficiency. 
            In this work, we present GarmentDiffusion, a new generative model capable of producing 
            centimeter-precise, vectorized 3D sewing patterns from multi-modal inputs (text, image, and incomplete sewing pattern).
            Our method efficiently encodes 3D sewing pattern parameters into compact edge token representations, 
            achieving a sequence length 10√ó shorter than that of the previous autoregressive modeling approach, i.e., DressCode.
            By employing a diffusion transformer, we simultaneously denoise all edge tokens along the temporal axis, 
            while maintaining a constant number of denoising steps regardless of dataset-specific edge and panel statistics.
            We achieve new state-of-the-art results on the largest parametric sewing pattern dataset, namely GarmentCodeData.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">
        ü™Ñ Tokenization
      </h2>
      <br>
      <!-- Your image here -->
      <div style="text-align: center;">
      <img src="static/images/tokenize.png" alt="MY ALT TEXT" width="600"/>
      <br>
      <br>
      <p style="text-align:left;">
        we encode edge-related parameters along the embedding dimension, 
        while denoising all edge tokens in parallel along the temporal axis.
        After rotating and translating the panels into 3D space, 
        each edge is represented by its starting endpoint, control point(s), arc parameters, stitch tag, and stitch flag. 
        The edges are arranged into a panel using zero-padding. 
        Each edge is assigned an edge-order index and a panel-order index 
        to indicate its position within the sequence and its association with the corresponding panel.
      </p>
      <br>
      <br>
    </div>

    <div class="container">
      <h2 class="title has-text-centered">
        üè† Network architecture
      </h2>
      <br>
      <!-- Your image here -->
      <div style="text-align: center;">
      <img src="static/images/network.png" alt="MY ALT TEXT" width="800"/>
      <br>
      <br>
      <p style="text-align:left;">
        During training, the edge sequence with injected noise is encoded 
        and combined with embeddings of edge-order indices, panel-order indices, and timesteps
        before being fed into the transformer network for prediction. 
        After embedding the text and image conditions, 
        decoupled cross-attention is used to compute the attention outputs separately 
        while sharing the query. 
        The results are then summed to achieve joint control of both text and image conditions.
        Automatic pattern completion is achieved by noise replacement with user-provided panel during inference.
      </p>
      <br>
      <br>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <h2 class="title has-text-centered">
    üéÜ Results demo
  </h2>
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!--BibTex citation -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{li2025garmentdiffusion,
  author    = {Li, Xinyu and Yao, Qi and Wang, Yuanda},
  title     = {GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers},
  booktitle = {Proceedings of the 34th International Joint Conference on Artificial Intelligence (IJCAI)},
  year      = {2025},
  url       = {https://arxiv.org/abs/2504.21476}
}</code></pre>
    </div>
  </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
